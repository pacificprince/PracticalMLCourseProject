---
title:  'Predicting Quality of Personal Activity using Machine Learning Algorithms'
author: Student
date: July 19, 2011
---
\  

```{r read_chunks, setup, echo=FALSE}
opts_chunk$set(echo=TRUE, cache=TRUE, warning=FALSE, messages=FALSE, eval=TRUE)
opts_chunk$set(comment=NA, fig.cap="", fig.path='figures/')
```

## Summary
****
Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is
now possible to collect a large amount of data about personal activity
relatively inexpensively. In this project, the
goal is to use data from accelerometers on the belt, forearm, arm, and
dumbell of 6 participants to quantify *how well* a particular activity is done. 
The participants were asked to perform barbell lifts
correctly and incorrectly in 5 different ways. More information is
available from the website here:
<http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight
Lifting Exercise Dataset).

## Data Loading 
****
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 
First the data is downloaded. On inspection it is found that
there is missing data denoted by `NA` or `#DIV/0!`. These are replaced by `NA`.

```{r download_data}
DownloadAndReadFile <- function(file.url, filename, nastrings){
  # Download training data if not already download
  if (!file.exists(filename)){
    download.file(file.url, destfile = filename)
    download.date <- date()  # Date stamp the downloaded data
  }
  
  read.csv(filename, header = TRUE, na.strings = nastrings)  
}

training.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testing.url  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

nastrings <- c("NA", "#DIV/0!")
data.training <- DownloadAndReadFile(training.url, "training.csv", nastrings)
data.testing  <- DownloadAndReadFile(testing.url,  "testing.csv",  nastrings)
```

The training data contains 19622 rows and 160 columns.
```{r dim_training}
dim(data.training)
```

And the testing data contains 20 rows and 160 columns
```{r dim_testing}
dim(data.testing)
```


## Data Pre-processing
****
As mentioned above, the *training* and *testing* data sets contain missing 
values. So columns with `NA` are removed in the datasets. Also, columns that
are not relevant to any sensor measurements are removed since these cannot be 
used to build a model.

### Feature Selection
```{r feature_select}
na.cols       <- colSums(!is.na(data.training)) == nrow(data.training)
data.training <- data.training[, na.cols]; 
data.testing  <- data.testing[, na.cols];  

rm.cols <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",  
             "cvtd_timestamp", "new_window",  "num_window")
data.training <- data.training[, !(colnames(data.training) %in% rm.cols)]
data.testing  <- data.testing[, !(colnames(data.testing) %in% rm.cols)]

```

The *training* and *testing* sets now have 53 columns. All columns 
in the training set are `numeric` or `int` except for `classe` (variable of interest) 
which is a `factor` (as desired). 
```{r str_training}
str(data.training)
```

### Training and Validation Sets 
The original *training* set is further subdivided into *training* and *validation* sets
for the purposes of modeling. The random number generator (RNG) state is (seed) is set to 
12321. The *training* set is partitioned so as to contain 70% of the original data
(13737 rows). The remainder of the data (5885 rows) is assigned to the `validation` set.
```{r split_data, results="hide"}
set.seed(12321)

library(caret)

inTrain <- createDataPartition(y = data.training$classe, p = 0.70, list = FALSE)

training   <- data.training[inTrain, ] 
validation <- data.training[-inTrain, ]

testing <- data.testing[, -ncol(data.testing)]
```

## Predictive Modeling
****
> Since the *classe* variable is a factor this is a **classification** problem.  

The strategy is to use the following four machine learning methods:

* Random Forest 
* Bagged CART
* Support Vector Machines with Radial Basis Function Kernel 
* k-Nearest Neighbors

Followed by an *ensemble* model. 

> Throughout this project, the *caret* package is used for training, 
> validation and testing the models.

A `models` object is used to store all information. The object contains the 
following parameters:  

--------------------------------------------------------------
Parameter                  Information
-------------------------- -----------------------------------
name                        List of model names

method                      Character strings corresponding to each
                            model in models\$name given as argument
                            to caret's `train` function 
            
fit                         Fit information returned from `train`      
                            function corresponding to each model in
                            models\$name 
            
pred\$validation            Predictions of models on validation set  

pred\$testing	              Predictions of models on testing set     

confmat	                    Confusion matrix for each model applied 
                            to the validation set                 
                            
confmat\$summary	          Dataframe of accuracy parameters from each model  

ensemble\$fit	              Fit information for *ensemble* model using the 
                            Random Forest algorithm        
                            
ensemble\$pred\$validation	Predictions of *ensemble* model on validation set 

ensemble\$pred\$testing	    Predictions of *ensemble* model on testing set   

ensemble\$confmat	          Confusion matrix for *ensemble* model 
                            applied to the validation set               
                
--------------------------------------------------------------

### FitModel Function
We begin by defining a generic function which contains training options common
to all models in the `train` function. This function takes the training data,
`data.train`, and the model method, `model.method` and applies the `train` function 
to create the model.

A *repeated cross-validation* scheme is used with 
*4 folds and 1 repeat*. PCA is used as a pre-processing algorithm and 
verbosity is turned off.
```{r funcdef_FitModel}
FitModel <- function(data.train, model.method = "rf"){
  
  trn.ctrl <- trainControl(method = 'repeatedcv', 
                           number = 4,            repeats = 1, 
                           returnResamp = 'none', classProbs = TRUE,
                           returnData = FALSE,    savePredictions = TRUE, 
                           verboseIter = FALSE,   allowParallel=TRUE)  
  
  model <- train(classe ~ ., data = data.train, method = model.method,
                 trControl = trn.ctrl, preProcess = c("pca"), 
                 verbose = FALSE, trace = FALSE)
  
  list(model)
  
}
```

### Train Models
With the FiModel function above, it is now simple to obtain a list of models to
the training data using `sapply` as shown below. All fit information is stored in 
the `models` object. Note that the RNG seed is set to 97611.

```{r fit_models, results="hide"}
library(randomForest)
library(ipred)
library(kernlab)
library(kknn)

set.seed(97611)

models <- c()

models$name   <- c("RandomForest", "BaggedCART", "SVMRadial", "kNearestNeighbors")
models$method <- c("rf", "treebag", "svmRadial", "kknn")

models$fit <- sapply(models$method, 
                     function(x) FitModel(training, model.method = x))

nmodels <- length(models$fit)
```

#### Predictions on Validation Set (Out-of-Sample)
Predictions for each model on the validation data set are obtained and saved in the
`models` object. These will be be compared to predictions from the *ensemble* later.
```{r pred_validation, results="hide"}
models$pred$validation <- sapply(seq(1, nmodels), 
                                 function(x) predict(models$fit[[x]], validation))
models$pred$validation <- as.data.frame(models$pred$validation)
colnames(models$pred$validation) <- models$name
```

#### Confusion Matrices
The confusion matrices corresponding to each model are obtained for the validation
data set. Again, these are saved in the `models` object for comparison with the 
corresponding confusion matrix from the *ensemble* model.
```{r confusion_matrix, results="hide"}
models$confmat  <- 
  sapply(seq(1, nmodels),
         function(x) 
           list(confusionMatrix(models$pred$validation[, x], validation$classe)))

models$confmat$summary <- sapply(seq(1, nmodels), 
                                 function(x) models$confmat[[x]]$overall)
models$confmat$summary <- as.data.frame(models$confmat$summary)
colnames(models$confmat$summary) <- models$name
```

### Ensemble: Combined Predictors
The *ensemble* model "stacks" the predictors obtained from the four machine learning
models used before. The fit, predictions on the validation set and the confusion
matrix are all stored in the `models$ensemble` object.
```{r ensemble_model, results="hide"}
ensemble.df <- cbind(models$pred$validation, classe = validation$classe)
models$ensemble$fit <- FitModel(ensemble.df, model.method = "rf")[[1]]

models$ensemble$pred$validation <- 
  predict(models$ensemble$fit, models$pred$validation)
models$ensemble$confmat <- 
  confusionMatrix(models$ensemble$pred$validation, validation$classe)
```

## Comparison
****
All models can now be compared. Since this is a classification problem, 
accuracy, sensitivity and specificity are used as metrics to assess the prediction
quality of each model.  

### Accuracy
As seen below, the *kNearestNeighbors* has the highest accuracy of individual 
models but the *ensemble* model has the highest accuracy. The lower and upper
bounds on accuracy are also summarized in the table and are tightest for the ensemble model.
```{r confmat_compare, results="asis"}
library(xtable)
confmat.summary <- cbind(models$confmat$summary, 
                         Ensemble = models$ensemble$confmat$overall)
print(xtable(confmat.summary), type = "html")
```

### Sensitivity
For each class, *kNearestNeighbors* and *ensemble* sensitivities are comparable though
the *ensemble* model seems to be *very* slightly better than *kNearestNeighbors* and the
best overall.
```{r sensitivity_summary, results="asis"}
library(xtable)
sensitivity.summary <- 
  cbind(sapply(seq(1, nmodels), function(x) models$confmat[[x]]$byClass[, 1]), 
      models$ensemble$confmat$byClass[, 1])
colnames(sensitivity.summary) <- c(models$name, "Ensemble")
print(xtable(sensitivity.summary), type = "html")
```

### Specificity
Again, the *ensemble* model seems to be have the highest specificity for each class
when compared to all other models.
```{r specificity_summary, results="asis"}
library(xtable)
specificity.summary <- 
  cbind(sapply(seq(1, nmodels), function(x) models$confmat[[x]]$byClass[, 2]), 
      models$ensemble$confmat$byClass[, 2])
colnames(specificity.summary) <- c(models$name, "Ensemble")
print(xtable(specificity.summary), type = "html")
```

## Predictions on Testing Data
****
All five models are used to predict the `classe` variable for the 20 observations
in the testing data set.
```{r pred_testing_models}
models$pred$testing    <- sapply(seq(1, nmodels), 
                                 function(x) predict(models$fit[[x]], testing))
models$pred$testing <- as.data.frame(models$pred$testing)
colnames(models$pred$testing) <- models$name

models$ensemble$pred$testing <- predict(models$ensemble$fit, models$pred$testing)
```

The table below shows that all models predict the exact same outcome for 
all the observations, except for the 3rd observation by *RandomForest* and the
3rd and 11th observations by *SVMRadial*. One can therefore have high confidence 
in the predictions.
```{r pred_summary_testing, results="asis"}
prediction.testing <- cbind(models$pred$testing, 
                            Ensemble = models$ensemble$pred$testing)
print(xtable(t(prediction.testing)), type = "html")
```


## Concluding Remarks
****
Four machine learning algorithms, RandomForest, BaggedCART, SVMRadial and 
kNearestNeighbors were used to model the training set. An ensemble model was created
using the predictions from the four algorithms. It was found that the *kNearestNeighbors*
has the highest accuracy of the individual algorithms but the *ensemble* model performed
slightly better overall. The ensemble model is used below as answers to predictions on the testing set.

```{r submission}
answers <- as.vector(prediction.testing$Ensemble)

pml_write_files = function(x) {
    n = length(x)
    for (i in 1:n) {
        filename = paste0("submission/problem_id_", i, ".txt")
        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, 
            col.names = FALSE)
    }
}

pml_write_files(answers)
```


## Document Information
****
This document was generated using knitr and pandoc

`knit('project.Rmd')`  

`system("pandoc project.md -o project.html -c custom.css --toc --self-contained --highlight-style=tango")`

```{r session_info}
sessionInfo()
```

